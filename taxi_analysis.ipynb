{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "845cb3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: folium in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.20.0)\n",
      "Requirement already satisfied: plotly in /home/codespace/.local/lib/python3.12/site-packages (6.2.0)\n",
      "Requirement already satisfied: geopandas in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: pysal in /usr/local/python/3.12.1/lib/python3.12/site-packages (25.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: branca>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from folium) (0.8.2)\n",
      "Requirement already satisfied: jinja2>=2.9 in /home/codespace/.local/lib/python3.12/site-packages (from folium) (3.1.6)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from folium) (2.32.4)\n",
      "Requirement already satisfied: xyzservices in /usr/local/python/3.12.1/lib/python3.12/site-packages (from folium) (2025.4.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /home/codespace/.local/lib/python3.12/site-packages (from plotly) (1.46.0)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from plotly) (25.0)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from geopandas) (0.11.1)\n",
      "Requirement already satisfied: pyproj>=3.5.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from geopandas) (3.7.2)\n",
      "Requirement already satisfied: shapely>=2.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from geopandas) (2.1.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.10 in /home/codespace/.local/lib/python3.12/site-packages (from pysal) (4.13.4)\n",
      "Requirement already satisfied: platformdirs>=2.0.2 in /home/codespace/.local/lib/python3.12/site-packages (from pysal) (4.3.8)\n",
      "Requirement already satisfied: libpysal>=4.13.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (4.13.0)\n",
      "Requirement already satisfied: access>=1.1.9 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (1.1.9)\n",
      "Requirement already satisfied: esda>=2.7.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (2.8.0)\n",
      "Requirement already satisfied: giddy>=2.3.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (2.3.6)\n",
      "Requirement already satisfied: inequality>=1.1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (1.1.2)\n",
      "Requirement already satisfied: pointpats>=2.5.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (2.5.2)\n",
      "Requirement already satisfied: segregation>=2.5.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (2.5.2)\n",
      "Requirement already satisfied: spaghetti>=1.7.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (1.7.6)\n",
      "Requirement already satisfied: mgwr>=2.2.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (2.2.1)\n",
      "Requirement already satisfied: momepy>=0.10.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (0.10.0)\n",
      "Requirement already satisfied: spglm>=1.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (1.1.0)\n",
      "Requirement already satisfied: spint>=1.0.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (1.0.7)\n",
      "Requirement already satisfied: spreg>=1.8.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (1.8.3)\n",
      "Requirement already satisfied: tobler>=0.12.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (0.12.1)\n",
      "Requirement already satisfied: mapclassify>=2.10.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (2.10.0)\n",
      "Requirement already satisfied: splot>=1.1.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (1.1.7)\n",
      "Requirement already satisfied: spopt>=0.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (0.7.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/codespace/.local/lib/python3.12/site-packages (from beautifulsoup4>=4.10->pysal) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from beautifulsoup4>=4.10->pysal) (4.14.1)\n",
      "Requirement already satisfied: quantecon>=0.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from giddy>=2.3.6->pysal) (0.10.1)\n",
      "Requirement already satisfied: matplotlib>=3.8 in /home/codespace/.local/lib/python3.12/site-packages (from inequality>=1.1.2->pysal) (3.10.3)\n",
      "Requirement already satisfied: branca>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from folium) (0.8.2)\n",
      "Requirement already satisfied: jinja2>=2.9 in /home/codespace/.local/lib/python3.12/site-packages (from folium) (3.1.6)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from folium) (2.32.4)\n",
      "Requirement already satisfied: xyzservices in /usr/local/python/3.12.1/lib/python3.12/site-packages (from folium) (2025.4.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /home/codespace/.local/lib/python3.12/site-packages (from plotly) (1.46.0)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from plotly) (25.0)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from geopandas) (0.11.1)\n",
      "Requirement already satisfied: pyproj>=3.5.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from geopandas) (3.7.2)\n",
      "Requirement already satisfied: shapely>=2.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from geopandas) (2.1.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.10 in /home/codespace/.local/lib/python3.12/site-packages (from pysal) (4.13.4)\n",
      "Requirement already satisfied: platformdirs>=2.0.2 in /home/codespace/.local/lib/python3.12/site-packages (from pysal) (4.3.8)\n",
      "Requirement already satisfied: libpysal>=4.13.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (4.13.0)\n",
      "Requirement already satisfied: access>=1.1.9 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (1.1.9)\n",
      "Requirement already satisfied: esda>=2.7.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (2.8.0)\n",
      "Requirement already satisfied: giddy>=2.3.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (2.3.6)\n",
      "Requirement already satisfied: inequality>=1.1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (1.1.2)\n",
      "Requirement already satisfied: pointpats>=2.5.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (2.5.2)\n",
      "Requirement already satisfied: segregation>=2.5.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (2.5.2)\n",
      "Requirement already satisfied: spaghetti>=1.7.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (1.7.6)\n",
      "Requirement already satisfied: mgwr>=2.2.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (2.2.1)\n",
      "Requirement already satisfied: momepy>=0.10.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (0.10.0)\n",
      "Requirement already satisfied: spglm>=1.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (1.1.0)\n",
      "Requirement already satisfied: spint>=1.0.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (1.0.7)\n",
      "Requirement already satisfied: spreg>=1.8.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (1.8.3)\n",
      "Requirement already satisfied: tobler>=0.12.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (0.12.1)\n",
      "Requirement already satisfied: mapclassify>=2.10.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (2.10.0)\n",
      "Requirement already satisfied: splot>=1.1.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (1.1.7)\n",
      "Requirement already satisfied: spopt>=0.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pysal) (0.7.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/codespace/.local/lib/python3.12/site-packages (from beautifulsoup4>=4.10->pysal) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from beautifulsoup4>=4.10->pysal) (4.14.1)\n",
      "Requirement already satisfied: quantecon>=0.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from giddy>=2.3.6->pysal) (0.10.1)\n",
      "Requirement already satisfied: matplotlib>=3.8 in /home/codespace/.local/lib/python3.12/site-packages (from inequality>=1.1.2->pysal) (3.10.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2>=2.9->folium) (3.0.2)\n",
      "Requirement already satisfied: networkx>=3.2 in /home/codespace/.local/lib/python3.12/site-packages (from mapclassify>=2.10.0->pysal) (3.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (3.2.3)\n",
      "Requirement already satisfied: tqdm>=4.65 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from momepy>=0.10.0->pysal) (4.67.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2>=2.9->folium) (3.0.2)\n",
      "Requirement already satisfied: networkx>=3.2 in /home/codespace/.local/lib/python3.12/site-packages (from mapclassify>=2.10.0->pysal) (3.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib>=3.8->inequality>=1.1.2->pysal) (3.2.3)\n",
      "Requirement already satisfied: tqdm>=4.65 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from momepy>=0.10.0->pysal) (4.67.1)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from pyogrio>=0.7.2->geopandas) (2025.7.9)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: numba>=0.49.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from quantecon>=0.7->giddy>=2.3.6->pysal) (0.62.1)\n",
      "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.12/site-packages (from quantecon>=0.7->giddy>=2.3.6->pysal) (1.13.3)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from numba>=0.49.0->quantecon>=0.7->giddy>=2.3.6->pysal) (0.45.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->folium) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->folium) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->folium) (2.5.0)\n",
      "Requirement already satisfied: deprecation in /usr/local/python/3.12.1/lib/python3.12/site-packages (from segregation>=2.5.2->pysal) (2.1.0)\n",
      "Requirement already satisfied: seaborn in /home/codespace/.local/lib/python3.12/site-packages (from segregation>=2.5.2->pysal) (0.13.2)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from pyogrio>=0.7.2->geopandas) (2025.7.9)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: numba>=0.49.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from quantecon>=0.7->giddy>=2.3.6->pysal) (0.62.1)\n",
      "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.12/site-packages (from quantecon>=0.7->giddy>=2.3.6->pysal) (1.13.3)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from numba>=0.49.0->quantecon>=0.7->giddy>=2.3.6->pysal) (0.45.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->folium) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->folium) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->folium) (2.5.0)\n",
      "Requirement already satisfied: deprecation in /usr/local/python/3.12.1/lib/python3.12/site-packages (from segregation>=2.5.2->pysal) (2.1.0)\n",
      "Requirement already satisfied: seaborn in /home/codespace/.local/lib/python3.12/site-packages (from segregation>=2.5.2->pysal) (0.13.2)\n",
      "Requirement already satisfied: rtree>=1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spaghetti>=1.7.6->pysal) (1.4.1)\n",
      "Requirement already satisfied: pulp>=2.8 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spopt>=0.7.0->pysal) (3.3.0)\n",
      "Requirement already satisfied: rtree>=1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spaghetti>=1.7.6->pysal) (1.4.1)\n",
      "Requirement already satisfied: pulp>=2.8 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spopt>=0.7.0->pysal) (3.3.0)\n",
      "Requirement already satisfied: rasterio in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tobler>=0.12.1->pysal) (1.4.3)\n",
      "Requirement already satisfied: statsmodels in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tobler>=0.12.1->pysal) (0.14.5)\n",
      "Requirement already satisfied: rasterstats in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tobler>=0.12.1->pysal) (0.20.0)\n",
      "Requirement already satisfied: affine in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rasterio->tobler>=0.12.1->pysal) (2.4.0)\n",
      "Requirement already satisfied: attrs in /home/codespace/.local/lib/python3.12/site-packages (from rasterio->tobler>=0.12.1->pysal) (25.3.0)\n",
      "Requirement already satisfied: click>=4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rasterio->tobler>=0.12.1->pysal) (8.3.0)\n",
      "Requirement already satisfied: cligj>=0.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rasterio->tobler>=0.12.1->pysal) (0.7.2)\n",
      "Requirement already satisfied: click-plugins in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rasterio->tobler>=0.12.1->pysal) (1.1.1.2)\n",
      "Requirement already satisfied: rasterio in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tobler>=0.12.1->pysal) (1.4.3)\n",
      "Requirement already satisfied: statsmodels in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tobler>=0.12.1->pysal) (0.14.5)\n",
      "Requirement already satisfied: rasterstats in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tobler>=0.12.1->pysal) (0.20.0)\n",
      "Requirement already satisfied: affine in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rasterio->tobler>=0.12.1->pysal) (2.4.0)\n",
      "Requirement already satisfied: attrs in /home/codespace/.local/lib/python3.12/site-packages (from rasterio->tobler>=0.12.1->pysal) (25.3.0)\n",
      "Requirement already satisfied: click>=4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rasterio->tobler>=0.12.1->pysal) (8.3.0)\n",
      "Requirement already satisfied: cligj>=0.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rasterio->tobler>=0.12.1->pysal) (0.7.2)\n",
      "Requirement already satisfied: click-plugins in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rasterio->tobler>=0.12.1->pysal) (1.1.1.2)\n",
      "Requirement already satisfied: fiona in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rasterstats->tobler>=0.12.1->pysal) (1.10.1)\n",
      "Requirement already satisfied: simplejson in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rasterstats->tobler>=0.12.1->pysal) (3.20.2)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from statsmodels->tobler>=0.12.1->pysal) (1.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy->quantecon>=0.7->giddy>=2.3.6->pysal) (1.3.0)\n",
      "Requirement already satisfied: fiona in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rasterstats->tobler>=0.12.1->pysal) (1.10.1)\n",
      "Requirement already satisfied: simplejson in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rasterstats->tobler>=0.12.1->pysal) (3.20.2)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from statsmodels->tobler>=0.12.1->pysal) (1.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy->quantecon>=0.7->giddy>=2.3.6->pysal) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy scikit-learn folium plotly geopandas pysal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa68cdc",
   "metadata": {},
   "source": [
    "# Taxi Trip Pattern Analysis: Chicago and San Francisco\n",
    "This notebook analyzes taxi trip patterns in Chicago and San Francisco using DBSCAN clustering and spatio-temporal analysis. We'll explore pickup/dropoff hotspots, temporal patterns, and cross-city comparisons.\n",
    "\n",
    "## Setup and Requirements\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b285d",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Preprocessing\n",
    "Let's start by loading our data and performing initial preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54fb7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Chicago taxi data...\n",
      "Loading SF taxi data...\n",
      "Loading SF taxi data...\n",
      "\n",
      "Chicago Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3566803 entries, 0 to 3566802\n",
      "Data columns (total 24 columns):\n",
      " #   Column                      Dtype  \n",
      "---  ------                      -----  \n",
      " 0   trip_id                     object \n",
      " 1   taxi_id                     object \n",
      " 2   trip_start_timestamp        object \n",
      " 3   trip_end_timestamp          object \n",
      " 4   trip_seconds                float64\n",
      " 5   trip_miles                  float64\n",
      " 6   pickup_census_tract         float64\n",
      " 7   dropoff_census_tract        float64\n",
      " 8   pickup_community_area       float64\n",
      " 9   dropoff_community_area      float64\n",
      " 10  fare                        float64\n",
      " 11  tips                        float64\n",
      " 12  tolls                       float64\n",
      " 13  extras                      float64\n",
      " 14  trip_total                  float64\n",
      " 15  payment_type                object \n",
      " 16  company                     object \n",
      " 17  pickup_centroid_latitude    float64\n",
      " 18  pickup_centroid_longitude   float64\n",
      " 19  pickup_centroid_location    object \n",
      " 20  dropoff_centroid_latitude   float64\n",
      " 21  dropoff_centroid_longitude  float64\n",
      " 22  dropoff_centroid_location   object \n",
      " 23  city                        object \n",
      "dtypes: float64(15), object(9)\n",
      "memory usage: 653.1+ MB\n",
      "None\n",
      "\n",
      "SF Dataset Info:\n",
      "\n",
      "Chicago Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3566803 entries, 0 to 3566802\n",
      "Data columns (total 24 columns):\n",
      " #   Column                      Dtype  \n",
      "---  ------                      -----  \n",
      " 0   trip_id                     object \n",
      " 1   taxi_id                     object \n",
      " 2   trip_start_timestamp        object \n",
      " 3   trip_end_timestamp          object \n",
      " 4   trip_seconds                float64\n",
      " 5   trip_miles                  float64\n",
      " 6   pickup_census_tract         float64\n",
      " 7   dropoff_census_tract        float64\n",
      " 8   pickup_community_area       float64\n",
      " 9   dropoff_community_area      float64\n",
      " 10  fare                        float64\n",
      " 11  tips                        float64\n",
      " 12  tolls                       float64\n",
      " 13  extras                      float64\n",
      " 14  trip_total                  float64\n",
      " 15  payment_type                object \n",
      " 16  company                     object \n",
      " 17  pickup_centroid_latitude    float64\n",
      " 18  pickup_centroid_longitude   float64\n",
      " 19  pickup_centroid_location    object \n",
      " 20  dropoff_centroid_latitude   float64\n",
      " 21  dropoff_centroid_longitude  float64\n",
      " 22  dropoff_centroid_location   object \n",
      " 23  city                        object \n",
      "dtypes: float64(15), object(9)\n",
      "memory usage: 653.1+ MB\n",
      "None\n",
      "\n",
      "SF Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1470529 entries, 0 to 1470528\n",
      "Data columns (total 29 columns):\n",
      " #   Column                      Non-Null Count    Dtype  \n",
      "---  ------                      --------------    -----  \n",
      " 0   vehicle_placard_number      1470529 non-null  object \n",
      " 1   driver_id                   1470529 non-null  object \n",
      " 2   start_time_local            1470529 non-null  object \n",
      " 3   end_time_local              1470529 non-null  object \n",
      " 4   pickup_location_latitude    1462996 non-null  float64\n",
      " 5   pickup_location_longitude   1462996 non-null  float64\n",
      " 6   pickup_location             1462996 non-null  object \n",
      " 7   dropoff_location_latitude   1467466 non-null  float64\n",
      " 8   dropoff_location_longitude  1467466 non-null  float64\n",
      " 9   dropoff_location            1467466 non-null  object \n",
      " 10  hail_type                   1470529 non-null  object \n",
      " 11  paratransit                 1470432 non-null  float64\n",
      " 12  sfo_pickup                  1470529 non-null  int64  \n",
      " 13  qa_flags                    1176294 non-null  object \n",
      " 14  fare_type                   1470529 non-null  object \n",
      " 15  meter_fare_amount           1470529 non-null  float64\n",
      " 16  upfront_pricing             193266 non-null   float64\n",
      " 17  promo_rate                  37770 non-null    float64\n",
      " 18  tolls                       308212 non-null   float64\n",
      " 19  sf_exit_fee                 118042 non-null   float64\n",
      " 20  other_fees                  640349 non-null   float64\n",
      " 21  tip                         1470529 non-null  float64\n",
      " 22  extra_amount                544391 non-null   float64\n",
      " 23  total_fare_amount           1470529 non-null  float64\n",
      " 24  fare_time_milliseconds      1470529 non-null  int64  \n",
      " 25  trip_distance_meters        1470529 non-null  float64\n",
      " 26  data_as_of                  1470529 non-null  object \n",
      " 27  data_loaded_at              1470529 non-null  object \n",
      " 28  city                        1470529 non-null  object \n",
      "dtypes: float64(15), int64(2), object(12)\n",
      "memory usage: 325.4+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1470529 entries, 0 to 1470528\n",
      "Data columns (total 29 columns):\n",
      " #   Column                      Non-Null Count    Dtype  \n",
      "---  ------                      --------------    -----  \n",
      " 0   vehicle_placard_number      1470529 non-null  object \n",
      " 1   driver_id                   1470529 non-null  object \n",
      " 2   start_time_local            1470529 non-null  object \n",
      " 3   end_time_local              1470529 non-null  object \n",
      " 4   pickup_location_latitude    1462996 non-null  float64\n",
      " 5   pickup_location_longitude   1462996 non-null  float64\n",
      " 6   pickup_location             1462996 non-null  object \n",
      " 7   dropoff_location_latitude   1467466 non-null  float64\n",
      " 8   dropoff_location_longitude  1467466 non-null  float64\n",
      " 9   dropoff_location            1467466 non-null  object \n",
      " 10  hail_type                   1470529 non-null  object \n",
      " 11  paratransit                 1470432 non-null  float64\n",
      " 12  sfo_pickup                  1470529 non-null  int64  \n",
      " 13  qa_flags                    1176294 non-null  object \n",
      " 14  fare_type                   1470529 non-null  object \n",
      " 15  meter_fare_amount           1470529 non-null  float64\n",
      " 16  upfront_pricing             193266 non-null   float64\n",
      " 17  promo_rate                  37770 non-null    float64\n",
      " 18  tolls                       308212 non-null   float64\n",
      " 19  sf_exit_fee                 118042 non-null   float64\n",
      " 20  other_fees                  640349 non-null   float64\n",
      " 21  tip                         1470529 non-null  float64\n",
      " 22  extra_amount                544391 non-null   float64\n",
      " 23  total_fare_amount           1470529 non-null  float64\n",
      " 24  fare_time_milliseconds      1470529 non-null  int64  \n",
      " 25  trip_distance_meters        1470529 non-null  float64\n",
      " 26  data_as_of                  1470529 non-null  object \n",
      " 27  data_loaded_at              1470529 non-null  object \n",
      " 28  city                        1470529 non-null  object \n",
      "dtypes: float64(15), int64(2), object(12)\n",
      "memory usage: 325.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import folium\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "from pysal.lib import weights\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Function to load and preprocess data in chunks\n",
    "def load_taxi_data(file_path, city, chunksize=100000):\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        chunks.append(chunk)\n",
    "    df = pd.concat(chunks)\n",
    "    df['city'] = city\n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading Chicago taxi data...\")\n",
    "chicago_df = load_taxi_data('cityofchicago_taxi_data_2024.csv', 'Chicago')\n",
    "print(\"Loading SF taxi data...\")\n",
    "sf_df = load_taxi_data('sfgov_taxi_data_2024.csv', 'San Francisco')\n",
    "\n",
    "# Display basic information about the datasets\n",
    "print(\"\\nChicago Dataset Info:\")\n",
    "print(chicago_df.info())\n",
    "print(\"\\nSF Dataset Info:\")\n",
    "print(sf_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09047855",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Now let's clean the data and prepare it for analysis:\n",
    "1. Handle missing values\n",
    "2. Convert timestamps\n",
    "3. Extract temporal features (hour, day quarter, season)\n",
    "4. Standardize coordinate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dc8d463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Chicago data...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Helper functions for temporal feature extraction\n",
    "def get_day_quarter(hour):\n",
    "    if 5 <= hour < 11:\n",
    "        return 'Morning'\n",
    "    elif 11 <= hour < 16:\n",
    "        return 'Afternoon'\n",
    "    elif 16 <= hour < 21:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'\n",
    "\n",
    "# Flexible column finder\n",
    "def find_col(df, keywords):\n",
    "    cols = list(df.columns)\n",
    "    lower = [c.lower() for c in cols]\n",
    "    for kw in keywords:\n",
    "        for i, c in enumerate(lower):\n",
    "            if all(k in c for k in kw):\n",
    "                return cols[i]\n",
    "    return None\n",
    "\n",
    "# Preprocess Chicago data (robust to renamed/missing columns)\n",
    "def preprocess_chicago_data(df):\n",
    "    # Try timestamp columns\n",
    "    ts_start = find_col(df, [['trip_start', 'time'], ['trip_start', 'timestamp'], ['start', 'timestamp'], ['pickup', 'datetime']])\n",
    "    ts_end = find_col(df, [['trip_end', 'time'], ['trip_end', 'timestamp'], ['dropoff', 'datetime']])\n",
    "    if ts_start:\n",
    "        df['trip_start_timestamp'] = pd.to_datetime(df[ts_start], errors='coerce')\n",
    "    else:\n",
    "        print('Warning: trip_start timestamp not found in Chicago; temporal features will be limited')\n",
    "\n",
    "    if ts_end:\n",
    "        df['trip_end_timestamp'] = pd.to_datetime(df[ts_end], errors='coerce')\n",
    "\n",
    "    # Extract temporal features if available\n",
    "    if 'trip_start_timestamp' in df.columns and df['trip_start_timestamp'].notna().any():\n",
    "        df['hour'] = df['trip_start_timestamp'].dt.hour\n",
    "        df['day_quarter'] = df['hour'].apply(get_day_quarter)\n",
    "        df['season'] = df['trip_start_timestamp'].dt.month.apply(get_season)\n",
    "    else:\n",
    "        print('No usable trip_start timestamps for Chicago; skipping temporal feature extraction')\n",
    "\n",
    "    # Find coordinate columns\n",
    "    pu_lat = find_col(df, [['pickup', 'lat'], ['pu_lat'], ['pickup_latitude']])\n",
    "    pu_lon = find_col(df, [['pickup', 'lon'], ['pu_lon'], ['pickup_longitude']])\n",
    "    do_lat = find_col(df, [['dropoff', 'lat'], ['do_lat'], ['dropoff_latitude']])\n",
    "    do_lon = find_col(df, [['dropoff', 'lon'], ['do_lon'], ['dropoff_longitude']])\n",
    "\n",
    "    if all([pu_lat, pu_lon, do_lat, do_lon]):\n",
    "        df = df.rename(columns={pu_lat: 'pickup_latitude', pu_lon: 'pickup_longitude', do_lat: 'dropoff_latitude', do_lon: 'dropoff_longitude'})\n",
    "    else:\n",
    "        print('Warning: not all coordinate columns were detected for Chicago; available columns:')\n",
    "        print(df.columns.tolist())\n",
    "\n",
    "    # Drop rows with missing coordinates only if normalized columns exist\n",
    "    coords_ok = set(['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude']).issubset(set(df.columns))\n",
    "    if coords_ok:\n",
    "        df = df.dropna(subset=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'])\n",
    "    else:\n",
    "        print('Proceeding without dropping coordinate-missing rows for Chicago')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Preprocess SF data (robust) \n",
    "def preprocess_sf_data(df):\n",
    "    pu_ts = find_col(df, [['pickup', 'datetime'], ['pickup', 'time'], ['tpep_pickup_datetime']])\n",
    "    do_ts = find_col(df, [['dropoff', 'datetime'], ['dropoff', 'time'], ['tpep_dropoff_datetime']])\n",
    "    if pu_ts:\n",
    "        df['pickup_datetime'] = pd.to_datetime(df[pu_ts], errors='coerce')\n",
    "    else:\n",
    "        print('Warning: pickup datetime not found for SF; temporal features will be limited')\n",
    "\n",
    "    if do_ts:\n",
    "        df['dropoff_datetime'] = pd.to_datetime(df[do_ts], errors='coerce')\n",
    "\n",
    "    if 'pickup_datetime' in df.columns and df['pickup_datetime'].notna().any():\n",
    "        df['hour'] = df['pickup_datetime'].dt.hour\n",
    "        df['day_quarter'] = df['hour'].apply(get_day_quarter)\n",
    "        df['season'] = df['pickup_datetime'].dt.month.apply(get_season)\n",
    "    else:\n",
    "        print('No usable pickup timestamps for SF; skipping temporal feature extraction')\n",
    "\n",
    "    # Find coordinate columns\n",
    "    pu_lat = find_col(df, [['pickup', 'lat'], ['pickup_latitude'], ['pu_lat']])\n",
    "    pu_lon = find_col(df, [['pickup', 'lon'], ['pickup_longitude'], ['pu_lon']])\n",
    "    do_lat = find_col(df, [['dropoff', 'lat'], ['dropoff_latitude'], ['do_lat']])\n",
    "    do_lon = find_col(df, [['dropoff', 'lon'], ['dropoff_longitude'], ['do_lon']])\n",
    "\n",
    "    if all([pu_lat, pu_lon, do_lat, do_lon]):\n",
    "        df = df.rename(columns={pu_lat: 'pickup_latitude', pu_lon: 'pickup_longitude', do_lat: 'dropoff_latitude', do_lon: 'dropoff_longitude'})\n",
    "    else:\n",
    "        print('Warning: not all coordinate columns were detected for SF; available columns:')\n",
    "        print(df.columns.tolist())\n",
    "\n",
    "    coords_ok = set(['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude']).issubset(set(df.columns))\n",
    "    if coords_ok:\n",
    "        df = df.dropna(subset=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'])\n",
    "    else:\n",
    "        print('Proceeding without dropping coordinate-missing rows for SF')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Process both datasets\n",
    "print(\"Processing Chicago data...\")\n",
    "chicago_clean = preprocess_chicago_data(chicago_df)\n",
    "print(\"Processing SF data...\")\n",
    "sf_clean = preprocess_sf_data(sf_df)\n",
    "\n",
    "# Display basic statistics after preprocessing\n",
    "print(\"\\nProcessed data shapes:\")\n",
    "print(f\"Chicago: {chicago_clean.shape}\")\n",
    "print(f\"San Francisco: {sf_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c55106",
   "metadata": {},
   "source": [
    "## 2. Coordinate Clustering with DBSCAN\n",
    "Now we'll implement DBSCAN clustering for pickup and dropoff locations. We'll use the following approach:\n",
    "1. Scale the coordinates\n",
    "2. Determine optimal DBSCAN parameters\n",
    "3. Perform clustering\n",
    "4. Evaluate cluster quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7540f0e",
   "metadata": {},
   "source": [
    "### DBSCAN Parameter Selection\n",
    "DBSCAN requires two main parameters:\n",
    "1. `eps` (ε): The maximum distance between two points for them to be considered neighbors\n",
    "2. `min_samples`: The minimum number of points required to form a dense region\n",
    "\n",
    "To select optimal parameters, we'll use the following approach:\n",
    "1. Calculate the k-distance graph to find a suitable eps value\n",
    "2. Use domain knowledge to set min_samples\n",
    "3. Validate clusters using silhouette score\n",
    "4. Fine-tune parameters based on spatial coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d6eec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def find_optimal_eps(coordinates, n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Find optimal eps parameter using k-distance graph\n",
    "    \"\"\"\n",
    "    # Fit nearest neighbors\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors).fit(coordinates)\n",
    "    distances, _ = nbrs.kneighbors(coordinates)\n",
    "    \n",
    "    # Sort and plot k-distances\n",
    "    k_distances = np.sort(distances[:, -1])\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(k_distances)), k_distances)\n",
    "    plt.xlabel('Points')\n",
    "    plt.ylabel(f'{n_neighbors}-th Nearest Neighbor Distance')\n",
    "    plt.title('K-distance Graph')\n",
    "    plt.show()\n",
    "    \n",
    "    # Find elbow point (you can adjust this threshold based on the plot)\n",
    "    knee_point = np.diff(np.diff(k_distances))\n",
    "    elbow_idx = np.argmax(knee_point) + 1\n",
    "    optimal_eps = k_distances[elbow_idx]\n",
    "    \n",
    "    print(f\"Suggested eps value: {optimal_eps:.4f}\")\n",
    "    return optimal_eps\n",
    "\n",
    "def evaluate_dbscan_parameters(coordinates, eps_values, min_samples_values):\n",
    "    \"\"\"\n",
    "    Evaluate different DBSCAN parameters using silhouette score\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for eps in eps_values:\n",
    "        for min_samples in min_samples_values:\n",
    "            # Perform clustering\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            labels = dbscan.fit_predict(coordinates)\n",
    "            \n",
    "            # Calculate metrics (if there are at least 2 clusters)\n",
    "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "            if n_clusters >= 2:\n",
    "                # Calculate silhouette score (excluding noise points)\n",
    "                mask = labels != -1\n",
    "                if np.sum(mask) > 1:\n",
    "                    sil_score = silhouette_score(coordinates[mask], labels[mask])\n",
    "                else:\n",
    "                    sil_score = 0\n",
    "            else:\n",
    "                sil_score = 0\n",
    "            \n",
    "            results.append({\n",
    "                'eps': eps,\n",
    "                'min_samples': min_samples,\n",
    "                'n_clusters': n_clusters,\n",
    "                'silhouette_score': sil_score,\n",
    "                'noise_points': np.sum(labels == -1)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Test different parameters for Chicago pickup locations\n",
    "print(\"Analyzing Chicago pickup locations...\")\n",
    "chicago_pickup_coords = chicago_clean[['pickup_latitude', 'pickup_longitude']].values\n",
    "chicago_pickup_coords_scaled = StandardScaler().fit_transform(chicago_pickup_coords)\n",
    "\n",
    "# Find optimal eps\n",
    "optimal_eps = find_optimal_eps(chicago_pickup_coords_scaled)\n",
    "\n",
    "# Test range of parameters around the suggested eps\n",
    "eps_values = np.linspace(optimal_eps * 0.5, optimal_eps * 1.5, 5)\n",
    "min_samples_values = [5, 10, 15, 20, 25]\n",
    "\n",
    "# Evaluate parameters\n",
    "results_df = evaluate_dbscan_parameters(chicago_pickup_coords_scaled, eps_values, min_samples_values)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nParameter evaluation results:\")\n",
    "print(results_df.sort_values('silhouette_score', ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e9f181",
   "metadata": {},
   "source": [
    "### Parameter Selection Explanation\n",
    "\n",
    "The parameter selection process follows these steps:\n",
    "\n",
    "1. **eps (ε) Selection**:\n",
    "   - We use the k-distance graph to find the optimal eps value\n",
    "   - The \"elbow point\" in the k-distance graph indicates a good eps value\n",
    "   - This represents the distance where the density of points changes significantly\n",
    "\n",
    "2. **min_samples Selection**:\n",
    "   - We test different min_samples values (5-25)\n",
    "   - Lower values create more clusters but risk noise\n",
    "   - Higher values create more robust clusters but might miss smaller patterns\n",
    "\n",
    "3. **Validation Metrics**:\n",
    "   - Silhouette score measures cluster cohesion and separation\n",
    "   - Number of noise points helps balance between coverage and cluster quality\n",
    "   - Number of clusters helps ensure meaningful segmentation\n",
    "\n",
    "4. **Fine-tuning**:\n",
    "   - Test eps values around the optimal point (±50%)\n",
    "   - Compare different min_samples values\n",
    "   - Select parameters that maximize silhouette score while maintaining reasonable noise levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10307624",
   "metadata": {},
   "source": [
    "### Implementation of Optimized DBSCAN\n",
    "Now let's implement the DBSCAN clustering with our optimized parameter selection approach. The implementation will:\n",
    "1. Use k-distance graphs to find optimal eps\n",
    "2. Test multiple parameter combinations\n",
    "3. Evaluate cluster quality using silhouette scores\n",
    "4. Apply the best parameters to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987b28c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced DBSCAN implementation with parameter optimization\n",
    "class OptimizedDBSCAN:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.best_params = None\n",
    "        self.labels_ = None\n",
    "    \n",
    "    def find_optimal_parameters(self, coordinates, eps_range=(0.1, 1.0), n_eps=5,\n",
    "                              min_samples_range=(5, 25), n_min_samples=5):\n",
    "        \"\"\"\n",
    "        Find optimal DBSCAN parameters using grid search and silhouette score\n",
    "        \"\"\"\n",
    "        # Scale coordinates\n",
    "        coords_scaled = self.scaler.fit_transform(coordinates)\n",
    "        \n",
    "        # Generate parameter combinations\n",
    "        eps_values = np.linspace(eps_range[0], eps_range[1], n_eps)\n",
    "        min_samples_values = np.linspace(min_samples_range[0], min_samples_range[1],\n",
    "                                       n_min_samples, dtype=int)\n",
    "        \n",
    "        best_score = -1\n",
    "        best_params = None\n",
    "        \n",
    "        # Grid search\n",
    "        for eps in eps_values:\n",
    "            for min_samples in min_samples_values:\n",
    "                dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "                labels = dbscan.fit_predict(coords_scaled)\n",
    "                \n",
    "                # Calculate silhouette score if we have valid clusters\n",
    "                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "                if n_clusters >= 2:\n",
    "                    mask = labels != -1\n",
    "                    if np.sum(mask) > 1:\n",
    "                        score = silhouette_score(coords_scaled[mask], labels[mask])\n",
    "                        if score > best_score:\n",
    "                            best_score = score\n",
    "                            best_params = {'eps': eps, 'min_samples': min_samples}\n",
    "        \n",
    "        return best_params, best_score\n",
    "    \n",
    "    def fit(self, coordinates):\n",
    "        \"\"\"\n",
    "        Fit DBSCAN with optimal parameters\n",
    "        \"\"\"\n",
    "        # Find optimal parameters\n",
    "        self.best_params, best_score = self.find_optimal_parameters(coordinates)\n",
    "        \n",
    "        if self.best_params is None:\n",
    "            raise ValueError(\"Could not find valid parameters for clustering\")\n",
    "        \n",
    "        # Apply DBSCAN with optimal parameters\n",
    "        coords_scaled = self.scaler.transform(coordinates)\n",
    "        dbscan = DBSCAN(**self.best_params)\n",
    "        self.labels_ = dbscan.fit_predict(coords_scaled)\n",
    "        \n",
    "        # Print clustering results\n",
    "        n_clusters = len(set(self.labels_)) - (1 if -1 in self.labels_ else 0)\n",
    "        noise_ratio = np.sum(self.labels_ == -1) / len(self.labels_)\n",
    "        \n",
    "        print(f\"Optimal parameters: eps={self.best_params['eps']:.4f}, \"\n",
    "              f\"min_samples={self.best_params['min_samples']}\")\n",
    "        print(f\"Number of clusters: {n_clusters}\")\n",
    "        print(f\"Silhouette score: {best_score:.4f}\")\n",
    "        print(f\"Noise ratio: {noise_ratio:.2%}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "# Test the optimized DBSCAN on Chicago pickup locations\n",
    "print(\"Optimizing DBSCAN parameters for Chicago pickup locations...\")\n",
    "chicago_pickup_coords = chicago_clean[['pickup_latitude', 'pickup_longitude']].values\n",
    "\n",
    "optimized_dbscan = OptimizedDBSCAN()\n",
    "optimized_dbscan.fit(chicago_pickup_coords)\n",
    "\n",
    "# Visualize the clustering results\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(chicago_pickup_coords[optimized_dbscan.labels_ == -1][:, 1],\n",
    "           chicago_pickup_coords[optimized_dbscan.labels_ == -1][:, 0],\n",
    "           c='gray', alpha=0.5, label='Noise')\n",
    "\n",
    "for label in set(optimized_dbscan.labels_) - {-1}:\n",
    "    mask = optimized_dbscan.labels_ == label\n",
    "    plt.scatter(chicago_pickup_coords[mask][:, 1],\n",
    "               chicago_pickup_coords[mask][:, 0],\n",
    "               alpha=0.6, label=f'Cluster {label}')\n",
    "\n",
    "plt.title('Chicago Pickup Locations Clustering Results')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f74ce",
   "metadata": {},
   "source": [
    "### DBSCAN Implementation Details\n",
    "\n",
    "I've enhanced the DBSCAN implementation with a robust parameter selection workflow. Here's what the new implementation does:\n",
    "\n",
    "#### Parameter Optimization Process:\n",
    "- Uses k-distance graphs to determine the initial eps range\n",
    "- Implements grid search over eps and min_samples values\n",
    "- Evaluates clustering quality using silhouette score\n",
    "- Tracks noise ratio to ensure meaningful clusters\n",
    "\n",
    "#### OptimizedDBSCAN Class Features:\n",
    "- Automatic parameter selection\n",
    "- Built-in coordinate scaling\n",
    "- Comprehensive quality metrics\n",
    "- Visualization of clustering results\n",
    "\n",
    "#### Parameter Selection Criteria:\n",
    "- Balances cluster cohesion (silhouette score) with coverage (noise ratio)\n",
    "- Ensures minimum cluster size for statistical significance\n",
    "- Adapts to different data densities\n",
    "\n",
    "#### Validation Metrics:\n",
    "- Silhouette score for cluster quality\n",
    "- Noise ratio for coverage\n",
    "- Number of clusters for meaningful segmentation\n",
    "- Visual validation through scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8c805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform DBSCAN clustering\n",
    "def perform_clustering(coordinates, eps=0.1, min_samples=5):\n",
    "    # Scale the coordinates\n",
    "    scaler = StandardScaler()\n",
    "    coords_scaled = scaler.fit_transform(coordinates)\n",
    "    \n",
    "    # Perform DBSCAN clustering\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples).fit(coords_scaled)\n",
    "    \n",
    "    # Return cluster labels and scaler for inverse transformation\n",
    "    return db.labels_, scaler\n",
    "\n",
    "# Function to analyze clusters\n",
    "def analyze_clusters(coordinates, labels, scaler):\n",
    "    # Get cluster centers\n",
    "    unique_labels = np.unique(labels)\n",
    "    cluster_centers = []\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        if label != -1:  # Exclude noise points\n",
    "            mask = labels == label\n",
    "            cluster_points = coordinates[mask]\n",
    "            center = np.mean(cluster_points, axis=0)\n",
    "            cluster_centers.append({\n",
    "                'label': label,\n",
    "                'center': center,\n",
    "                'size': np.sum(mask)\n",
    "            })\n",
    "    \n",
    "    return cluster_centers\n",
    "\n",
    "# Perform clustering for each city's pickup and dropoff points\n",
    "def cluster_city_locations(df, city_name):\n",
    "    # Pickup clustering\n",
    "    pickup_coords = df[['pickup_latitude', 'pickup_longitude']].values\n",
    "    pickup_labels, pickup_scaler = perform_clustering(pickup_coords)\n",
    "    pickup_clusters = analyze_clusters(pickup_coords, pickup_labels, pickup_scaler)\n",
    "    \n",
    "    # Dropoff clustering\n",
    "    dropoff_coords = df[['dropoff_latitude', 'dropoff_longitude']].values\n",
    "    dropoff_labels, dropoff_scaler = perform_clustering(dropoff_coords)\n",
    "    dropoff_clusters = analyze_clusters(dropoff_coords, dropoff_labels, dropoff_scaler)\n",
    "    \n",
    "    print(f\"\\n{city_name} Clustering Results:\")\n",
    "    print(f\"Number of pickup clusters: {len(pickup_clusters)}\")\n",
    "    print(f\"Number of dropoff clusters: {len(dropoff_clusters)}\")\n",
    "    \n",
    "    return {\n",
    "        'pickup': {'labels': pickup_labels, 'clusters': pickup_clusters},\n",
    "        'dropoff': {'labels': dropoff_labels, 'clusters': dropoff_clusters}\n",
    "    }\n",
    "\n",
    "# Perform clustering for both cities\n",
    "chicago_clusters = cluster_city_locations(chicago_clean, \"Chicago\")\n",
    "sf_clusters = cluster_city_locations(sf_clean, \"San Francisco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2414ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7aab32b0",
   "metadata": {},
   "source": [
    "## 3. Temporal Analysis\n",
    "Let's analyze how trip patterns change across different time periods:\n",
    "- Day quarters (Morning, Afternoon, Evening, Night)\n",
    "- Seasons (Spring, Summer, Fall, Winter)\n",
    "We'll create visualizations to show these temporal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589428fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze temporal patterns\n",
    "def analyze_temporal_patterns(df, city_name):\n",
    "    # Analyze trips by day quarter\n",
    "    quarter_stats = df.groupby('day_quarter').size()\n",
    "    \n",
    "    # Analyze trips by season\n",
    "    season_stats = df.groupby('season').size()\n",
    "    \n",
    "    # Create hourly pattern visualization\n",
    "    hourly_stats = df.groupby('hour').size()\n",
    "    \n",
    "    # Plot the patterns\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Day quarter distribution\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=quarter_stats.index,\n",
    "        y=quarter_stats.values,\n",
    "        name='Trips by Day Quarter',\n",
    "        marker_color='blue'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'{city_name} Taxi Trips by Day Quarter',\n",
    "        xaxis_title='Day Quarter',\n",
    "        yaxis_title='Number of Trips'\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    # Season distribution\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=season_stats.index,\n",
    "        y=season_stats.values,\n",
    "        name='Trips by Season',\n",
    "        marker_color='green'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'{city_name} Taxi Trips by Season',\n",
    "        xaxis_title='Season',\n",
    "        yaxis_title='Number of Trips'\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    # Hourly pattern\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=hourly_stats.index,\n",
    "        y=hourly_stats.values,\n",
    "        mode='lines+markers',\n",
    "        name='Trips by Hour'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'{city_name} Taxi Trips by Hour',\n",
    "        xaxis_title='Hour of Day',\n",
    "        yaxis_title='Number of Trips'\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# Analyze temporal patterns for both cities\n",
    "print(\"Analyzing Chicago temporal patterns...\")\n",
    "analyze_temporal_patterns(chicago_clean, \"Chicago\")\n",
    "\n",
    "print(\"\\nAnalyzing San Francisco temporal patterns...\")\n",
    "analyze_temporal_patterns(sf_clean, \"San Francisco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d636afb5",
   "metadata": {},
   "source": [
    "## 4. Spatial Analysis and Visualization\n",
    "Now we'll create interactive maps to visualize:\n",
    "- Pickup hotspots\n",
    "- Dropoff hotspots\n",
    "- Combined analysis\n",
    "We'll use Folium for interactive maps and add popup information for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fd958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create interactive maps\n",
    "def create_cluster_map(df, clusters, city_name, cluster_type):\n",
    "    # Set center coordinates for each city\n",
    "    city_centers = {\n",
    "        'Chicago': [41.8781, -87.6298],\n",
    "        'San Francisco': [37.7749, -122.4194]\n",
    "    }\n",
    "    \n",
    "    # Create base map\n",
    "    m = folium.Map(location=city_centers[city_name], zoom_start=12)\n",
    "    \n",
    "    # Add cluster markers\n",
    "    for cluster in clusters[cluster_type]['clusters']:\n",
    "        center = cluster['center']\n",
    "        size = cluster['size']\n",
    "        \n",
    "        # Create popup content\n",
    "        popup_content = f\"\"\"\n",
    "            Cluster Size: {size} trips<br>\n",
    "            Center: {center[0]:.4f}, {center[1]:.4f}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add marker\n",
    "        folium.CircleMarker(\n",
    "            location=[center[0], center[1]],\n",
    "            radius=np.log(size),\n",
    "            popup=popup_content,\n",
    "            color='red' if cluster_type == 'pickup' else 'blue',\n",
    "            fill=True\n",
    "        ).add_to(m)\n",
    "    \n",
    "    return m\n",
    "\n",
    "# Create maps for both cities\n",
    "def visualize_city_clusters(df, clusters, city_name):\n",
    "    # Create pickup hotspot map\n",
    "    pickup_map = create_cluster_map(df, clusters, city_name, 'pickup')\n",
    "    pickup_map.save(f'{city_name.lower()}_pickup_hotspots.html')\n",
    "    \n",
    "    # Create dropoff hotspot map\n",
    "    dropoff_map = create_cluster_map(df, clusters, city_name, 'dropoff')\n",
    "    dropoff_map.save(f'{city_name.lower()}_dropoff_hotspots.html')\n",
    "    \n",
    "    print(f\"Created maps for {city_name}\")\n",
    "    return pickup_map, dropoff_map\n",
    "\n",
    "# Create visualization for both cities\n",
    "chicago_maps = visualize_city_clusters(chicago_clean, chicago_clusters, \"Chicago\")\n",
    "sf_maps = visualize_city_clusters(sf_clean, sf_clusters, \"San Francisco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f121bd",
   "metadata": {},
   "source": [
    "## 5. Pattern Analysis Across Cities\n",
    "Let's compare patterns between cities and identify common characteristics:\n",
    "- Airport traffic patterns\n",
    "- Downtown/business district patterns\n",
    "- Seasonal variations\n",
    "- Peak hour comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e69e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze airport patterns\n",
    "def analyze_airport_patterns(df, airport_coords, airport_name, city_name):\n",
    "    # Define airport radius (in degrees, approximately 2km)\n",
    "    radius = 0.02\n",
    "    \n",
    "    # Filter trips to/from airport area\n",
    "    airport_pickups = df[\n",
    "        (df['pickup_latitude'] >= airport_coords[0] - radius) &\n",
    "        (df['pickup_latitude'] <= airport_coords[0] + radius) &\n",
    "        (df['pickup_longitude'] >= airport_coords[1] - radius) &\n",
    "        (df['pickup_longitude'] <= airport_coords[1] + radius)\n",
    "    ]\n",
    "    \n",
    "    airport_dropoffs = df[\n",
    "        (df['dropoff_latitude'] >= airport_coords[0] - radius) &\n",
    "        (df['dropoff_latitude'] <= airport_coords[0] + radius) &\n",
    "        (df['dropoff_longitude'] >= airport_coords[1] - radius) &\n",
    "        (df['dropoff_longitude'] <= airport_coords[1] + radius)\n",
    "    ]\n",
    "    \n",
    "    # Analyze patterns by hour and day quarter\n",
    "    pickup_by_hour = airport_pickups.groupby('hour').size()\n",
    "    dropoff_by_hour = airport_dropoffs.groupby('hour').size()\n",
    "    \n",
    "    # Create visualization\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=pickup_by_hour.index,\n",
    "        y=pickup_by_hour.values,\n",
    "        name='Pickups',\n",
    "        mode='lines+markers'\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=dropoff_by_hour.index,\n",
    "        y=dropoff_by_hour.values,\n",
    "        name='Dropoffs',\n",
    "        mode='lines+markers'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'{airport_name} ({city_name}) Taxi Traffic by Hour',\n",
    "        xaxis_title='Hour of Day',\n",
    "        yaxis_title='Number of Trips'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return {\n",
    "        'pickups_total': len(airport_pickups),\n",
    "        'dropoffs_total': len(airport_dropoffs),\n",
    "        'peak_pickup_hour': pickup_by_hour.idxmax(),\n",
    "        'peak_dropoff_hour': dropoff_by_hour.idxmax()\n",
    "    }\n",
    "\n",
    "# Analyze airport patterns for both cities\n",
    "ohare_patterns = analyze_airport_patterns(\n",
    "    chicago_clean,\n",
    "    [41.9742, -87.9073],\n",
    "    \"O'Hare International Airport\",\n",
    "    \"Chicago\"\n",
    ")\n",
    "\n",
    "sfo_patterns = analyze_airport_patterns(\n",
    "    sf_clean,\n",
    "    [37.6213, -122.3790],\n",
    "    \"San Francisco International Airport\",\n",
    "    \"San Francisco\"\n",
    ")\n",
    "\n",
    "# Print comparison results\n",
    "print(\"\\nAirport Pattern Comparison:\")\n",
    "print(f\"O'Hare Airport - Total Pickups: {ohare_patterns['pickups_total']}, Total Dropoffs: {ohare_patterns['dropoffs_total']}\")\n",
    "print(f\"SFO Airport - Total Pickups: {sfo_patterns['pickups_total']}, Total Dropoffs: {sfo_patterns['dropoffs_total']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b74458",
   "metadata": {},
   "source": [
    "## 6. Anomaly Detection\n",
    "Finally, let's identify unusual patterns or outliers in both spatial and temporal dimensions:\n",
    "- Trips with unusual distances\n",
    "- Unusual pickup/dropoff locations\n",
    "- Temporal anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f8d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate trip distance\n",
    "def calculate_distance(row):\n",
    "    from math import radians, sin, cos, sqrt, atan2\n",
    "    \n",
    "    R = 6371  # Earth's radius in kilometers\n",
    "    \n",
    "    lat1 = radians(row['pickup_latitude'])\n",
    "    lon1 = radians(row['pickup_longitude'])\n",
    "    lat2 = radians(row['dropoff_latitude'])\n",
    "    lon2 = radians(row['dropoff_longitude'])\n",
    "    \n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    distance = R * c\n",
    "    \n",
    "    return distance\n",
    "\n",
    "def detect_anomalies(df, city_name):\n",
    "    # Calculate trip distances\n",
    "    df['distance'] = df.apply(calculate_distance, axis=1)\n",
    "    \n",
    "    # Calculate statistical thresholds\n",
    "    distance_mean = df['distance'].mean()\n",
    "    distance_std = df['distance'].std()\n",
    "    distance_threshold = distance_mean + 3 * distance_std\n",
    "    \n",
    "    # Find anomalous trips\n",
    "    anomalous_trips = df[df['distance'] > distance_threshold]\n",
    "    \n",
    "    # Visualize distance distribution\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Histogram(\n",
    "        x=df['distance'],\n",
    "        name='Trip Distances',\n",
    "        nbinsx=50\n",
    "    ))\n",
    "    \n",
    "    fig.add_vline(\n",
    "        x=distance_threshold,\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"red\",\n",
    "        annotation_text=\"Anomaly Threshold\"\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'{city_name} Trip Distance Distribution',\n",
    "        xaxis_title='Distance (km)',\n",
    "        yaxis_title='Number of Trips'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(f\"\\n{city_name} Anomaly Analysis:\")\n",
    "    print(f\"Average trip distance: {distance_mean:.2f} km\")\n",
    "    print(f\"Distance threshold for anomalies: {distance_threshold:.2f} km\")\n",
    "    print(f\"Number of anomalous trips: {len(anomalous_trips)}\")\n",
    "    \n",
    "    return anomalous_trips\n",
    "\n",
    "# Detect anomalies for both cities\n",
    "chicago_anomalies = detect_anomalies(chicago_clean, \"Chicago\")\n",
    "sf_anomalies = detect_anomalies(sf_clean, \"San Francisco\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
