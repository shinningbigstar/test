{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845cb3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy scikit-learn folium plotly geopandas pysal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa68cdc",
   "metadata": {},
   "source": [
    "# Taxi Trip Pattern Analysis: Chicago and San Francisco\n",
    "This notebook analyzes taxi trip patterns in Chicago and San Francisco using DBSCAN clustering and spatio-temporal analysis. We'll explore pickup/dropoff hotspots, temporal patterns, and cross-city comparisons.\n",
    "\n",
    "## Setup and Requirements\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b285d",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Preprocessing\n",
    "Let's start by loading our data and performing initial preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54fb7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import folium\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "from pysal.lib import weights\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Function to load and preprocess data in chunks\n",
    "def load_taxi_data(file_path, city, chunksize=100000):\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "        chunks.append(chunk)\n",
    "    df = pd.concat(chunks)\n",
    "    df['city'] = city\n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading Chicago taxi data...\")\n",
    "chicago_df = load_taxi_data('cityofchicago_taxi_data_2024.csv', 'Chicago')\n",
    "print(\"Loading SF taxi data...\")\n",
    "sf_df = load_taxi_data('sfgov_taxi_data_2024.csv', 'San Francisco')\n",
    "\n",
    "# Display basic information about the datasets\n",
    "print(\"\\nChicago Dataset Info:\")\n",
    "print(chicago_df.info())\n",
    "print(\"\\nSF Dataset Info:\")\n",
    "print(sf_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09047855",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Now let's clean the data and prepare it for analysis:\n",
    "1. Handle missing values\n",
    "2. Convert timestamps\n",
    "3. Extract temporal features (hour, day quarter, season)\n",
    "4. Standardize coordinate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc8d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for temporal feature extraction\n",
    "def get_day_quarter(hour):\n",
    "    if 5 <= hour < 11:\n",
    "        return 'Morning'\n",
    "    elif 11 <= hour < 16:\n",
    "        return 'Afternoon'\n",
    "    elif 16 <= hour < 21:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'\n",
    "\n",
    "# Preprocess Chicago data\n",
    "def preprocess_chicago_data(df):\n",
    "    # Convert timestamps\n",
    "    df['trip_start_timestamp'] = pd.to_datetime(df['trip_start_timestamp'])\n",
    "    df['trip_end_timestamp'] = pd.to_datetime(df['trip_end_timestamp'])\n",
    "    \n",
    "    # Extract temporal features\n",
    "    df['hour'] = df['trip_start_timestamp'].dt.hour\n",
    "    df['day_quarter'] = df['hour'].apply(get_day_quarter)\n",
    "    df['season'] = df['trip_start_timestamp'].dt.month.apply(get_season)\n",
    "    \n",
    "    # Handle missing coordinates\n",
    "    df = df.dropna(subset=['pickup_latitude', 'pickup_longitude', \n",
    "                          'dropoff_latitude', 'dropoff_longitude'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Preprocess SF data\n",
    "def preprocess_sf_data(df):\n",
    "    # Convert timestamps\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])\n",
    "    \n",
    "    # Extract temporal features\n",
    "    df['hour'] = df['pickup_datetime'].dt.hour\n",
    "    df['day_quarter'] = df['hour'].apply(get_day_quarter)\n",
    "    df['season'] = df['pickup_datetime'].dt.month.apply(get_season)\n",
    "    \n",
    "    # Handle missing coordinates\n",
    "    df = df.dropna(subset=['pickup_latitude', 'pickup_longitude', \n",
    "                          'dropoff_latitude', 'dropoff_longitude'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process both datasets\n",
    "print(\"Processing Chicago data...\")\n",
    "chicago_clean = preprocess_chicago_data(chicago_df)\n",
    "print(\"Processing SF data...\")\n",
    "sf_clean = preprocess_sf_data(sf_df)\n",
    "\n",
    "# Display basic statistics after preprocessing\n",
    "print(\"\\nProcessed data shapes:\")\n",
    "print(f\"Chicago: {chicago_clean.shape}\")\n",
    "print(f\"San Francisco: {sf_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c55106",
   "metadata": {},
   "source": [
    "## 2. Coordinate Clustering with DBSCAN\n",
    "Now we'll implement DBSCAN clustering for pickup and dropoff locations. We'll use the following approach:\n",
    "1. Scale the coordinates\n",
    "2. Determine optimal DBSCAN parameters\n",
    "3. Perform clustering\n",
    "4. Evaluate cluster quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7540f0e",
   "metadata": {},
   "source": [
    "### DBSCAN Parameter Selection\n",
    "DBSCAN requires two main parameters:\n",
    "1. `eps` (Îµ): The maximum distance between two points for them to be considered neighbors\n",
    "2. `min_samples`: The minimum number of points required to form a dense region\n",
    "\n",
    "To select optimal parameters, we'll use the following approach:\n",
    "1. Calculate the k-distance graph to find a suitable eps value\n",
    "2. Use domain knowledge to set min_samples\n",
    "3. Validate clusters using silhouette score\n",
    "4. Fine-tune parameters based on spatial coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d6eec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def find_optimal_eps(coordinates, n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Find optimal eps parameter using k-distance graph\n",
    "    \"\"\"\n",
    "    # Fit nearest neighbors\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors).fit(coordinates)\n",
    "    distances, _ = nbrs.kneighbors(coordinates)\n",
    "    \n",
    "    # Sort and plot k-distances\n",
    "    k_distances = np.sort(distances[:, -1])\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(k_distances)), k_distances)\n",
    "    plt.xlabel('Points')\n",
    "    plt.ylabel(f'{n_neighbors}-th Nearest Neighbor Distance')\n",
    "    plt.title('K-distance Graph')\n",
    "    plt.show()\n",
    "    \n",
    "    # Find elbow point (you can adjust this threshold based on the plot)\n",
    "    knee_point = np.diff(np.diff(k_distances))\n",
    "    elbow_idx = np.argmax(knee_point) + 1\n",
    "    optimal_eps = k_distances[elbow_idx]\n",
    "    \n",
    "    print(f\"Suggested eps value: {optimal_eps:.4f}\")\n",
    "    return optimal_eps\n",
    "\n",
    "def evaluate_dbscan_parameters(coordinates, eps_values, min_samples_values):\n",
    "    \"\"\"\n",
    "    Evaluate different DBSCAN parameters using silhouette score\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for eps in eps_values:\n",
    "        for min_samples in min_samples_values:\n",
    "            # Perform clustering\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            labels = dbscan.fit_predict(coordinates)\n",
    "            \n",
    "            # Calculate metrics (if there are at least 2 clusters)\n",
    "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "            if n_clusters >= 2:\n",
    "                # Calculate silhouette score (excluding noise points)\n",
    "                mask = labels != -1\n",
    "                if np.sum(mask) > 1:\n",
    "                    sil_score = silhouette_score(coordinates[mask], labels[mask])\n",
    "                else:\n",
    "                    sil_score = 0\n",
    "            else:\n",
    "                sil_score = 0\n",
    "            \n",
    "            results.append({\n",
    "                'eps': eps,\n",
    "                'min_samples': min_samples,\n",
    "                'n_clusters': n_clusters,\n",
    "                'silhouette_score': sil_score,\n",
    "                'noise_points': np.sum(labels == -1)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Test different parameters for Chicago pickup locations\n",
    "print(\"Analyzing Chicago pickup locations...\")\n",
    "chicago_pickup_coords = chicago_clean[['pickup_latitude', 'pickup_longitude']].values\n",
    "chicago_pickup_coords_scaled = StandardScaler().fit_transform(chicago_pickup_coords)\n",
    "\n",
    "# Find optimal eps\n",
    "optimal_eps = find_optimal_eps(chicago_pickup_coords_scaled)\n",
    "\n",
    "# Test range of parameters around the suggested eps\n",
    "eps_values = np.linspace(optimal_eps * 0.5, optimal_eps * 1.5, 5)\n",
    "min_samples_values = [5, 10, 15, 20, 25]\n",
    "\n",
    "# Evaluate parameters\n",
    "results_df = evaluate_dbscan_parameters(chicago_pickup_coords_scaled, eps_values, min_samples_values)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nParameter evaluation results:\")\n",
    "print(results_df.sort_values('silhouette_score', ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e9f181",
   "metadata": {},
   "source": [
    "### Parameter Selection Explanation\n",
    "\n",
    "The parameter selection process follows these steps:\n",
    "\n",
    "1. **eps (Îµ) Selection**:\n",
    "   - We use the k-distance graph to find the optimal eps value\n",
    "   - The \"elbow point\" in the k-distance graph indicates a good eps value\n",
    "   - This represents the distance where the density of points changes significantly\n",
    "\n",
    "2. **min_samples Selection**:\n",
    "   - We test different min_samples values (5-25)\n",
    "   - Lower values create more clusters but risk noise\n",
    "   - Higher values create more robust clusters but might miss smaller patterns\n",
    "\n",
    "3. **Validation Metrics**:\n",
    "   - Silhouette score measures cluster cohesion and separation\n",
    "   - Number of noise points helps balance between coverage and cluster quality\n",
    "   - Number of clusters helps ensure meaningful segmentation\n",
    "\n",
    "4. **Fine-tuning**:\n",
    "   - Test eps values around the optimal point (Â±50%)\n",
    "   - Compare different min_samples values\n",
    "   - Select parameters that maximize silhouette score while maintaining reasonable noise levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10307624",
   "metadata": {},
   "source": [
    "### Implementation of Optimized DBSCAN\n",
    "Now let's implement the DBSCAN clustering with our optimized parameter selection approach. The implementation will:\n",
    "1. Use k-distance graphs to find optimal eps\n",
    "2. Test multiple parameter combinations\n",
    "3. Evaluate cluster quality using silhouette scores\n",
    "4. Apply the best parameters to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987b28c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced DBSCAN implementation with parameter optimization\n",
    "class OptimizedDBSCAN:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.best_params = None\n",
    "        self.labels_ = None\n",
    "    \n",
    "    def find_optimal_parameters(self, coordinates, eps_range=(0.1, 1.0), n_eps=5,\n",
    "                              min_samples_range=(5, 25), n_min_samples=5):\n",
    "        \"\"\"\n",
    "        Find optimal DBSCAN parameters using grid search and silhouette score\n",
    "        \"\"\"\n",
    "        # Scale coordinates\n",
    "        coords_scaled = self.scaler.fit_transform(coordinates)\n",
    "        \n",
    "        # Generate parameter combinations\n",
    "        eps_values = np.linspace(eps_range[0], eps_range[1], n_eps)\n",
    "        min_samples_values = np.linspace(min_samples_range[0], min_samples_range[1],\n",
    "                                       n_min_samples, dtype=int)\n",
    "        \n",
    "        best_score = -1\n",
    "        best_params = None\n",
    "        \n",
    "        # Grid search\n",
    "        for eps in eps_values:\n",
    "            for min_samples in min_samples_values:\n",
    "                dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "                labels = dbscan.fit_predict(coords_scaled)\n",
    "                \n",
    "                # Calculate silhouette score if we have valid clusters\n",
    "                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "                if n_clusters >= 2:\n",
    "                    mask = labels != -1\n",
    "                    if np.sum(mask) > 1:\n",
    "                        score = silhouette_score(coords_scaled[mask], labels[mask])\n",
    "                        if score > best_score:\n",
    "                            best_score = score\n",
    "                            best_params = {'eps': eps, 'min_samples': min_samples}\n",
    "        \n",
    "        return best_params, best_score\n",
    "    \n",
    "    def fit(self, coordinates):\n",
    "        \"\"\"\n",
    "        Fit DBSCAN with optimal parameters\n",
    "        \"\"\"\n",
    "        # Find optimal parameters\n",
    "        self.best_params, best_score = self.find_optimal_parameters(coordinates)\n",
    "        \n",
    "        if self.best_params is None:\n",
    "            raise ValueError(\"Could not find valid parameters for clustering\")\n",
    "        \n",
    "        # Apply DBSCAN with optimal parameters\n",
    "        coords_scaled = self.scaler.transform(coordinates)\n",
    "        dbscan = DBSCAN(**self.best_params)\n",
    "        self.labels_ = dbscan.fit_predict(coords_scaled)\n",
    "        \n",
    "        # Print clustering results\n",
    "        n_clusters = len(set(self.labels_)) - (1 if -1 in self.labels_ else 0)\n",
    "        noise_ratio = np.sum(self.labels_ == -1) / len(self.labels_)\n",
    "        \n",
    "        print(f\"Optimal parameters: eps={self.best_params['eps']:.4f}, \"\n",
    "              f\"min_samples={self.best_params['min_samples']}\")\n",
    "        print(f\"Number of clusters: {n_clusters}\")\n",
    "        print(f\"Silhouette score: {best_score:.4f}\")\n",
    "        print(f\"Noise ratio: {noise_ratio:.2%}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "# Test the optimized DBSCAN on Chicago pickup locations\n",
    "print(\"Optimizing DBSCAN parameters for Chicago pickup locations...\")\n",
    "chicago_pickup_coords = chicago_clean[['pickup_latitude', 'pickup_longitude']].values\n",
    "\n",
    "optimized_dbscan = OptimizedDBSCAN()\n",
    "optimized_dbscan.fit(chicago_pickup_coords)\n",
    "\n",
    "# Visualize the clustering results\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(chicago_pickup_coords[optimized_dbscan.labels_ == -1][:, 1],\n",
    "           chicago_pickup_coords[optimized_dbscan.labels_ == -1][:, 0],\n",
    "           c='gray', alpha=0.5, label='Noise')\n",
    "\n",
    "for label in set(optimized_dbscan.labels_) - {-1}:\n",
    "    mask = optimized_dbscan.labels_ == label\n",
    "    plt.scatter(chicago_pickup_coords[mask][:, 1],\n",
    "               chicago_pickup_coords[mask][:, 0],\n",
    "               alpha=0.6, label=f'Cluster {label}')\n",
    "\n",
    "plt.title('Chicago Pickup Locations Clustering Results')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f74ce",
   "metadata": {},
   "source": [
    "### DBSCAN Implementation Details\n",
    "\n",
    "I've enhanced the DBSCAN implementation with a robust parameter selection workflow. Here's what the new implementation does:\n",
    "\n",
    "#### Parameter Optimization Process:\n",
    "- Uses k-distance graphs to determine the initial eps range\n",
    "- Implements grid search over eps and min_samples values\n",
    "- Evaluates clustering quality using silhouette score\n",
    "- Tracks noise ratio to ensure meaningful clusters\n",
    "\n",
    "#### OptimizedDBSCAN Class Features:\n",
    "- Automatic parameter selection\n",
    "- Built-in coordinate scaling\n",
    "- Comprehensive quality metrics\n",
    "- Visualization of clustering results\n",
    "\n",
    "#### Parameter Selection Criteria:\n",
    "- Balances cluster cohesion (silhouette score) with coverage (noise ratio)\n",
    "- Ensures minimum cluster size for statistical significance\n",
    "- Adapts to different data densities\n",
    "\n",
    "#### Validation Metrics:\n",
    "- Silhouette score for cluster quality\n",
    "- Noise ratio for coverage\n",
    "- Number of clusters for meaningful segmentation\n",
    "- Visual validation through scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8c805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform DBSCAN clustering\n",
    "def perform_clustering(coordinates, eps=0.1, min_samples=5):\n",
    "    # Scale the coordinates\n",
    "    scaler = StandardScaler()\n",
    "    coords_scaled = scaler.fit_transform(coordinates)\n",
    "    \n",
    "    # Perform DBSCAN clustering\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples).fit(coords_scaled)\n",
    "    \n",
    "    # Return cluster labels and scaler for inverse transformation\n",
    "    return db.labels_, scaler\n",
    "\n",
    "# Function to analyze clusters\n",
    "def analyze_clusters(coordinates, labels, scaler):\n",
    "    # Get cluster centers\n",
    "    unique_labels = np.unique(labels)\n",
    "    cluster_centers = []\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        if label != -1:  # Exclude noise points\n",
    "            mask = labels == label\n",
    "            cluster_points = coordinates[mask]\n",
    "            center = np.mean(cluster_points, axis=0)\n",
    "            cluster_centers.append({\n",
    "                'label': label,\n",
    "                'center': center,\n",
    "                'size': np.sum(mask)\n",
    "            })\n",
    "    \n",
    "    return cluster_centers\n",
    "\n",
    "# Perform clustering for each city's pickup and dropoff points\n",
    "def cluster_city_locations(df, city_name):\n",
    "    # Pickup clustering\n",
    "    pickup_coords = df[['pickup_latitude', 'pickup_longitude']].values\n",
    "    pickup_labels, pickup_scaler = perform_clustering(pickup_coords)\n",
    "    pickup_clusters = analyze_clusters(pickup_coords, pickup_labels, pickup_scaler)\n",
    "    \n",
    "    # Dropoff clustering\n",
    "    dropoff_coords = df[['dropoff_latitude', 'dropoff_longitude']].values\n",
    "    dropoff_labels, dropoff_scaler = perform_clustering(dropoff_coords)\n",
    "    dropoff_clusters = analyze_clusters(dropoff_coords, dropoff_labels, dropoff_scaler)\n",
    "    \n",
    "    print(f\"\\n{city_name} Clustering Results:\")\n",
    "    print(f\"Number of pickup clusters: {len(pickup_clusters)}\")\n",
    "    print(f\"Number of dropoff clusters: {len(dropoff_clusters)}\")\n",
    "    \n",
    "    return {\n",
    "        'pickup': {'labels': pickup_labels, 'clusters': pickup_clusters},\n",
    "        'dropoff': {'labels': dropoff_labels, 'clusters': dropoff_clusters}\n",
    "    }\n",
    "\n",
    "# Perform clustering for both cities\n",
    "chicago_clusters = cluster_city_locations(chicago_clean, \"Chicago\")\n",
    "sf_clusters = cluster_city_locations(sf_clean, \"San Francisco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2414ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7aab32b0",
   "metadata": {},
   "source": [
    "## 3. Temporal Analysis\n",
    "Let's analyze how trip patterns change across different time periods:\n",
    "- Day quarters (Morning, Afternoon, Evening, Night)\n",
    "- Seasons (Spring, Summer, Fall, Winter)\n",
    "We'll create visualizations to show these temporal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589428fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze temporal patterns\n",
    "def analyze_temporal_patterns(df, city_name):\n",
    "    # Analyze trips by day quarter\n",
    "    quarter_stats = df.groupby('day_quarter').size()\n",
    "    \n",
    "    # Analyze trips by season\n",
    "    season_stats = df.groupby('season').size()\n",
    "    \n",
    "    # Create hourly pattern visualization\n",
    "    hourly_stats = df.groupby('hour').size()\n",
    "    \n",
    "    # Plot the patterns\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Day quarter distribution\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=quarter_stats.index,\n",
    "        y=quarter_stats.values,\n",
    "        name='Trips by Day Quarter',\n",
    "        marker_color='blue'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'{city_name} Taxi Trips by Day Quarter',\n",
    "        xaxis_title='Day Quarter',\n",
    "        yaxis_title='Number of Trips'\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    # Season distribution\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=season_stats.index,\n",
    "        y=season_stats.values,\n",
    "        name='Trips by Season',\n",
    "        marker_color='green'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'{city_name} Taxi Trips by Season',\n",
    "        xaxis_title='Season',\n",
    "        yaxis_title='Number of Trips'\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    # Hourly pattern\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=hourly_stats.index,\n",
    "        y=hourly_stats.values,\n",
    "        mode='lines+markers',\n",
    "        name='Trips by Hour'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'{city_name} Taxi Trips by Hour',\n",
    "        xaxis_title='Hour of Day',\n",
    "        yaxis_title='Number of Trips'\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# Analyze temporal patterns for both cities\n",
    "print(\"Analyzing Chicago temporal patterns...\")\n",
    "analyze_temporal_patterns(chicago_clean, \"Chicago\")\n",
    "\n",
    "print(\"\\nAnalyzing San Francisco temporal patterns...\")\n",
    "analyze_temporal_patterns(sf_clean, \"San Francisco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d636afb5",
   "metadata": {},
   "source": [
    "## 4. Spatial Analysis and Visualization\n",
    "Now we'll create interactive maps to visualize:\n",
    "- Pickup hotspots\n",
    "- Dropoff hotspots\n",
    "- Combined analysis\n",
    "We'll use Folium for interactive maps and add popup information for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fd958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create interactive maps\n",
    "def create_cluster_map(df, clusters, city_name, cluster_type):\n",
    "    # Set center coordinates for each city\n",
    "    city_centers = {\n",
    "        'Chicago': [41.8781, -87.6298],\n",
    "        'San Francisco': [37.7749, -122.4194]\n",
    "    }\n",
    "    \n",
    "    # Create base map\n",
    "    m = folium.Map(location=city_centers[city_name], zoom_start=12)\n",
    "    \n",
    "    # Add cluster markers\n",
    "    for cluster in clusters[cluster_type]['clusters']:\n",
    "        center = cluster['center']\n",
    "        size = cluster['size']\n",
    "        \n",
    "        # Create popup content\n",
    "        popup_content = f\"\"\"\n",
    "            Cluster Size: {size} trips<br>\n",
    "            Center: {center[0]:.4f}, {center[1]:.4f}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add marker\n",
    "        folium.CircleMarker(\n",
    "            location=[center[0], center[1]],\n",
    "            radius=np.log(size),\n",
    "            popup=popup_content,\n",
    "            color='red' if cluster_type == 'pickup' else 'blue',\n",
    "            fill=True\n",
    "        ).add_to(m)\n",
    "    \n",
    "    return m\n",
    "\n",
    "# Create maps for both cities\n",
    "def visualize_city_clusters(df, clusters, city_name):\n",
    "    # Create pickup hotspot map\n",
    "    pickup_map = create_cluster_map(df, clusters, city_name, 'pickup')\n",
    "    pickup_map.save(f'{city_name.lower()}_pickup_hotspots.html')\n",
    "    \n",
    "    # Create dropoff hotspot map\n",
    "    dropoff_map = create_cluster_map(df, clusters, city_name, 'dropoff')\n",
    "    dropoff_map.save(f'{city_name.lower()}_dropoff_hotspots.html')\n",
    "    \n",
    "    print(f\"Created maps for {city_name}\")\n",
    "    return pickup_map, dropoff_map\n",
    "\n",
    "# Create visualization for both cities\n",
    "chicago_maps = visualize_city_clusters(chicago_clean, chicago_clusters, \"Chicago\")\n",
    "sf_maps = visualize_city_clusters(sf_clean, sf_clusters, \"San Francisco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f121bd",
   "metadata": {},
   "source": [
    "## 5. Pattern Analysis Across Cities\n",
    "Let's compare patterns between cities and identify common characteristics:\n",
    "- Airport traffic patterns\n",
    "- Downtown/business district patterns\n",
    "- Seasonal variations\n",
    "- Peak hour comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e69e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze airport patterns\n",
    "def analyze_airport_patterns(df, airport_coords, airport_name, city_name):\n",
    "    # Define airport radius (in degrees, approximately 2km)\n",
    "    radius = 0.02\n",
    "    \n",
    "    # Filter trips to/from airport area\n",
    "    airport_pickups = df[\n",
    "        (df['pickup_latitude'] >= airport_coords[0] - radius) &\n",
    "        (df['pickup_latitude'] <= airport_coords[0] + radius) &\n",
    "        (df['pickup_longitude'] >= airport_coords[1] - radius) &\n",
    "        (df['pickup_longitude'] <= airport_coords[1] + radius)\n",
    "    ]\n",
    "    \n",
    "    airport_dropoffs = df[\n",
    "        (df['dropoff_latitude'] >= airport_coords[0] - radius) &\n",
    "        (df['dropoff_latitude'] <= airport_coords[0] + radius) &\n",
    "        (df['dropoff_longitude'] >= airport_coords[1] - radius) &\n",
    "        (df['dropoff_longitude'] <= airport_coords[1] + radius)\n",
    "    ]\n",
    "    \n",
    "    # Analyze patterns by hour and day quarter\n",
    "    pickup_by_hour = airport_pickups.groupby('hour').size()\n",
    "    dropoff_by_hour = airport_dropoffs.groupby('hour').size()\n",
    "    \n",
    "    # Create visualization\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=pickup_by_hour.index,\n",
    "        y=pickup_by_hour.values,\n",
    "        name='Pickups',\n",
    "        mode='lines+markers'\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=dropoff_by_hour.index,\n",
    "        y=dropoff_by_hour.values,\n",
    "        name='Dropoffs',\n",
    "        mode='lines+markers'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'{airport_name} ({city_name}) Taxi Traffic by Hour',\n",
    "        xaxis_title='Hour of Day',\n",
    "        yaxis_title='Number of Trips'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return {\n",
    "        'pickups_total': len(airport_pickups),\n",
    "        'dropoffs_total': len(airport_dropoffs),\n",
    "        'peak_pickup_hour': pickup_by_hour.idxmax(),\n",
    "        'peak_dropoff_hour': dropoff_by_hour.idxmax()\n",
    "    }\n",
    "\n",
    "# Analyze airport patterns for both cities\n",
    "ohare_patterns = analyze_airport_patterns(\n",
    "    chicago_clean,\n",
    "    [41.9742, -87.9073],\n",
    "    \"O'Hare International Airport\",\n",
    "    \"Chicago\"\n",
    ")\n",
    "\n",
    "sfo_patterns = analyze_airport_patterns(\n",
    "    sf_clean,\n",
    "    [37.6213, -122.3790],\n",
    "    \"San Francisco International Airport\",\n",
    "    \"San Francisco\"\n",
    ")\n",
    "\n",
    "# Print comparison results\n",
    "print(\"\\nAirport Pattern Comparison:\")\n",
    "print(f\"O'Hare Airport - Total Pickups: {ohare_patterns['pickups_total']}, Total Dropoffs: {ohare_patterns['dropoffs_total']}\")\n",
    "print(f\"SFO Airport - Total Pickups: {sfo_patterns['pickups_total']}, Total Dropoffs: {sfo_patterns['dropoffs_total']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b74458",
   "metadata": {},
   "source": [
    "## 6. Anomaly Detection\n",
    "Finally, let's identify unusual patterns or outliers in both spatial and temporal dimensions:\n",
    "- Trips with unusual distances\n",
    "- Unusual pickup/dropoff locations\n",
    "- Temporal anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f8d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate trip distance\n",
    "def calculate_distance(row):\n",
    "    from math import radians, sin, cos, sqrt, atan2\n",
    "    \n",
    "    R = 6371  # Earth's radius in kilometers\n",
    "    \n",
    "    lat1 = radians(row['pickup_latitude'])\n",
    "    lon1 = radians(row['pickup_longitude'])\n",
    "    lat2 = radians(row['dropoff_latitude'])\n",
    "    lon2 = radians(row['dropoff_longitude'])\n",
    "    \n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    distance = R * c\n",
    "    \n",
    "    return distance\n",
    "\n",
    "def detect_anomalies(df, city_name):\n",
    "    # Calculate trip distances\n",
    "    df['distance'] = df.apply(calculate_distance, axis=1)\n",
    "    \n",
    "    # Calculate statistical thresholds\n",
    "    distance_mean = df['distance'].mean()\n",
    "    distance_std = df['distance'].std()\n",
    "    distance_threshold = distance_mean + 3 * distance_std\n",
    "    \n",
    "    # Find anomalous trips\n",
    "    anomalous_trips = df[df['distance'] > distance_threshold]\n",
    "    \n",
    "    # Visualize distance distribution\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Histogram(\n",
    "        x=df['distance'],\n",
    "        name='Trip Distances',\n",
    "        nbinsx=50\n",
    "    ))\n",
    "    \n",
    "    fig.add_vline(\n",
    "        x=distance_threshold,\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"red\",\n",
    "        annotation_text=\"Anomaly Threshold\"\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'{city_name} Trip Distance Distribution',\n",
    "        xaxis_title='Distance (km)',\n",
    "        yaxis_title='Number of Trips'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(f\"\\n{city_name} Anomaly Analysis:\")\n",
    "    print(f\"Average trip distance: {distance_mean:.2f} km\")\n",
    "    print(f\"Distance threshold for anomalies: {distance_threshold:.2f} km\")\n",
    "    print(f\"Number of anomalous trips: {len(anomalous_trips)}\")\n",
    "    \n",
    "    return anomalous_trips\n",
    "\n",
    "# Detect anomalies for both cities\n",
    "chicago_anomalies = detect_anomalies(chicago_clean, \"Chicago\")\n",
    "sf_anomalies = detect_anomalies(sf_clean, \"San Francisco\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
